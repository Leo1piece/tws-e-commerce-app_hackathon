# ğŸ›ï¸ EasyShop - Modern E-commerce Platform

[![Next.js](https://img.shields.io/badge/Next.js-14.1.0-black?style=flat-square&logo=next.js)](https://nextjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0.0-blue?style=flat-square&logo=typescript)](https://www.typescriptlang.org/)
[![MongoDB](https://img.shields.io/badge/MongoDB-8.1.1-green?style=flat-square&logo=mongodb)](https://www.mongodb.com/)
[![Redux](https://img.shields.io/badge/Redux-2.2.1-purple?style=flat-square&logo=redux)](https://redux.js.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

EasyShop is a modern, full-stack e-commerce platform built with Next.js 14, TypeScript, and MongoDB. It features a beautiful UI with Tailwind CSS, secure authentication, real-time cart updates, and a seamless shopping experience.

## âœ¨ Features

- ğŸ¨ Modern and responsive UI with dark mode support
- ğŸ” Secure JWT-based authentication
- ğŸ›’ Real-time cart management with Redux
- ğŸ“± Mobile-first design approach
- ğŸ” Advanced product search and filtering
- ğŸ’³ Secure checkout process
- ğŸ“¦ Multiple product categories
- ğŸ‘¤ User profiles and order history
- ğŸŒ™ Dark/Light theme support

## ğŸ—ï¸ Architecture

EasyShop follows a three-tier architecture pattern:

### 1. Presentation Tier (Frontend)
- Next.js React Components
- Redux for State Management
- Tailwind CSS for Styling
- Client-side Routing
- Responsive UI Components



### 2. Application Tier (Backend)
- Next.js API Routes
- Business Logic
- Authentication & Authorization
- Request Validation
- Error Handling
- Data Processing

### 3. Data Tier (Database)
- MongoDB Database
- Mongoose ODM
- Data Models
- CRUD Operations
- Data Validation

## PreRequisites

> [!IMPORTANT]  
> Before you begin setting up this project, make sure the following tools are installed and configured properly on your system:

## Setup & Initialization <br/>

### 1. Install Terraform
* Install Terraform<br/>
#### Linux & macOS
```bash
curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
sudo apt-get update && sudo apt-get install terraform
```
### Verify Installation
```bash
terraform -v
```
### Initialize Terraform
```bash
terraform init
```
### 2. Install AWS CLI
AWS CLI (Command Line Interface) allows you to interact with AWS services directly from the command line.

```bash
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
sudo apt install unzip
unzip awscliv2.zip
sudo ./aws/install
```
###  Install AWS CLI in Windows 'powershell'
```msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi```


 ```aws configure```


> #### This will prompt you to enter:<br/>
- **AWS Access Key ID:**<br/>
- **AWS Secret Access Key:**<br/>
- **Default region name:**<br/>
- **Default output format:**<br/>

> [!NOTE] 
> Make sure the IAM user you're using has the necessary permissions. Youâ€™ll need an AWS IAM Role with programmatic access enabled, along with the Access Key and Secret Key.

## Getting Started

> Follow the steps below to get your infrastructure up and running using Terraform:<br/>

0 ssl /tls

# Deploying a Secure Nginx Website on AWS

This guide explains how to deploy an **Nginx web server** on an **EC2 instance** behind a **Load Balancer**, configure **Route 53 DNS**, and secure the website with an **SSL/TLS certificate** from AWS Certificate Manager (ACM).

---

## Steps

### 0.1 Launch EC2 Instance
![alt text](image-26.png)

### 0.2 Set Up Load Balancer
- Create a **Load Balancer** (Application Load Balancer recommended).
- Create a **Listener** and add the EC2 instance into the **Target Group**.


load balancer çš„ sg é…ç½® ï¼Œè¦ä¸ç„¶ï¼Œè®¿é—®æ— æ³•æˆåŠŸ
ALB å®‰å…¨ç»„ (SG)

å…¥ç«™è¦å…è®¸ï¼š

TCP 80ï¼ˆHTTPï¼‰æ¥æº 0.0.0.0/0

TCP 443ï¼ˆHTTPSï¼Œå¦‚ä½¿ç”¨ï¼‰æ¥æº 0.0.0.0/0

å‡ºç«™ä¸€èˆ¬ä¿æŒ All traffic å…è®¸å³å¯ï¼ˆé»˜è®¤æ˜¯å…è®¸ï¼‰ã€‚
### 0.3 Configure DNS in Route 53
- Create a **Public Hosted Zone** in **Route 53**.

hotst name. hostingger.


### 0.4 Update Domain Registrar
- Go to your **domain registrar** (e.g., Hostinger, GoDaddy).
- Copy the **nameservers** from Route 53 and update them in the registrar.
ä¸»è¦æ˜¯å¾—ä¿®æ”¹ nameserver
how the dns work 
top level domain, will tranct to autoriaztive server,  æ å¼€å§‹ï¼Œæˆ‘çš„ name server æ˜¯ogdaday,ä½†æ˜¯ç°åœ¨ autoraize server æ˜¯ route 53.ç°åœ¨å‘Šè¯‰go daddy, when the request is coming, you dont need to hold it and rout the traffic to route 53.
ç„¶åä½ çš„browerå°±çŸ¥é“ server çš„ip.
å† è‡ªå·±çš„ ec2ä¸Š è¿è¡Œ  host devops.site èƒ½çœ‹åˆ° ip æ‰€ä»¥åºå·change  nameserver
å¯ä»¥å» dsn checker æ¥æŸ¥çœ‹ ä¹Ÿå¯ä»¥ä½¿ç”¨nslookup æ¥æŸ¥çœ‹

TLS TERmeation means the traffic will be encyped.
![alt text](image-22.png)

### 0.5 Wait for Propagation
- DNS propagation may take **12â€“48 hours**.

### 0.6 Request SSL/TLS Certificate
- Once DNS propagation is complete:
  - Go to **AWS Certificate Manager (ACM)**.
  - Request a **public certificate** for your domain.
  - Add the required CNAME record in **Route 53**.

![alt text](image-27.png)
ç„¶å ç‚¹å‡»createcords in route 53
![alt text](image-25.png)
### 0.7 Validate Certificate
- Wait for validation (usually **5â€“6 minutes**).
- The certificate will then be issued.

### 0.8 Configure HTTPS in Load Balancer
- Go to the **Load Balancer**.
- Create a new **HTTPS listener** or edit the existing listener to use the SSL certificate.

### 0.9 Update DNS Records
- In Route 53, create or edit an **A record / CNAME record** pointing your domain to the Load Balancer.

### 0.10 Access Your Website
- Open a browser and access your domain with:


1. **Clone the Repository:**
First, clone this repo to your local machine:<br/>
```bash
git clone https://github.com/LondheShubham153/tws-e-commerce-app.git
cd terraform
```
2. **Generate SSH Key Pair:**
Create a new SSH key to access your EC2 instance:
```bash
ssh-keygen -f terra-key
```
This will prompt you to create a new key file named terra-key.

3. **Private key permission:** Change your private key permission:
```bash
chmod 400 terra-key
```

4. **Initialize Terraform:**
Initialize the Terraform working directory to download required providers:
```bash
terraform init
```
5. **Review the Execution Plan:**
Before applying changes, always check the execution plan:
```bash
terraform plan
```
6. **Apply the Configuration:**
Now, apply the changes and create the infrastructure:
```bash
terraform apply
```
> Confirm with `yes` when prompted.

7. **Access Your EC2 Instance;** <br/>
After deployment, grab the public IP of your EC2 instance from the output or AWS Console, then connect using SSH:
```bash
ssh -i terra-key ubuntu@<public-ip>
```
8. **Update your kubeconfig:**
wherever you want to access your eks wheather it is yur local machine or bastion server this command will help you to interact with your eks.
> [!CAUTION]
> you need to configure aws cli first to execute this command:

```bash
aws configure
```

```bash
aws eks --region ap-southeast-2 update-kubeconfig --name tws-eks-cluster
```

åªæœ‰vpc å†…éƒ¨çš„ bastion å¯ä»¥access eks control plane
9. **Check your cluster:**
```bash
kubectl get nodes
```

## Jenkins Setup Steps
> [!TIP]
> Check if jenkins service is running:

```bash
sudo systemctl status jenkins
```
## Steps to Access Jenkins & Install Plugins

#### 1. **Open Jenkins in Browser:**
> Use your public IP with port 8080:
>**http://<public_IP>:8080**

#### 2. **Initial Admin password:**
> Start the service and get the Jenkins initial admin password:
> ```bash
> sudo cat /var/lib/jenkins/secrets/initialAdminPassword
> ```

#### 3. **Start Jenkins (*If Not Running*):**
> Get the Jenkins initial admin password:
> ```bash
> sudo systemctl enable jenkins
> sudo systemctl restart jenkins
> ```
#### 4. **Install Essential Plugins:**
> - Navigate to:
> **Manage Jenkins â†’ Plugins â†’ Available Plugins**<br/>
> - Search and install the following:<br/>
>   - **Docker Pipeline**<br/>
>   - **Pipeline View**


#### 5. **Set Up Docker & GitHub Credentials in Jenkins (Global Credentials)**<br/>
>
> - GitHub Credentials:
>   - Go to:
**Jenkins â†’ Manage Jenkins â†’ Credentials â†’ (Global) â†’ Add Credentials**
> - Use:
>   - Kind: **Username with password**
>   - ID: **github-credentials**<br/>


> - DockerHub Credentials:
> Go to the same Global Credentials section
> - Use:
>   - Kind: **Username with password**
>   - ID: **docker-hub-credentials**
> [Notes:]
> Use these IDs in your Jenkins pipeline for secure access to GitHub and DockerHub

#### 6. Jenkins Shared Library Setup:
> - `Configure Trusted Pipeline Library`:
>   - Go to:
> **Jenkins â†’ Manage Jenkins â†’ Configure System**
> Scroll to Global Pipeline Libraries section
>
> - **Add a New Shared Library:** 
> - **Name:** Shared
> - **Default Version:** main
> - **Project Repository URL:** `https://github.com/<your user-name/jenkins-shared-libraries`.
>
> [Notes:] 
> Make sure the repo contains a proper directory structure eq: vars/<br/>
	var é‡Œé¢ çš„ repoå’Œ docker éœ€è¦ä¿®æ”¹ä¸€ä¸‹
#### 7. Setup Pipeline<br/>
> - Create New Pipeline Job<br/>
>   - **Name:** EasyShop<br/>
>   - **Type:** Pipeline<br/>
> Press `Okey`<br/>

> > In **General**<br/>
> > - **Description:** EasyShop<br/>
> > - **Check the box:** `GitHub project`<br/>
> > - **GitHub Repo URL:** `https://github.com/<your user-name/tws-e-commerce-app`<br/>
>
> > In **Trigger**<br/>
> > - **Check the box:**`GitHub hook trigger for GITScm polling`<br/>
>
> > In **Pipeline**<br/>
> > - **Definition:** `Pipeline script from SCM`<br/>
> > - **SCM:** `Git`<br/>
> > - **Repository URL:** `https://github.com/<your user-name/tws-e-commerce-app`<br/>
> > - **Credentials:** `github-credentials`<br/>
> > - **Branch:** master<br/>
> > - **Script Path:** `Jenkinsfile`<br/>

#### **Fork Required Repos**<br/>
> > Fork App Repo:<br/>
> > * Open the `Jenkinsfile`<br/>
> > * Change the DockerHub username to yours<br/>
>
> > **Fork Shared Library Repo:**<br/>
> > * Edit `vars/update_k8s_manifest.groovy`<br/>
> > * Update with your `DockerHub username`<br/>
> 
> > **Setup Webhook**<br/> ä¸ºä»€ä¹ˆè¦set up webhook? éœ€è¦ç ”ç©¶ä¸€ä¸‹
> > In GitHub:<br/>
> >  * Go to **`Settings` â†’ `Webhooks`**<br/>
> >  * Add a new webhook pointing to your Jenkins URL<br/>
> >  * Select: **`GitHub hook trigger for GITScm polling`** in Jenkins job<br/>
>
> > **Trigger the Pipeline**<br/>
> > Click **`Build Now`** in Jenkins

#### **8. CD â€“ Continuous Deployment Setup**<br/>
**Prerequisites:**<br/>
Before configuring CD, make sure the following tools are installed:<br/>
* Installations Required:<br/>
**kubectl**<br/>
**AWS CLI**

**SSH into Bastion Server**<br/>
* Connect to your Bastion EC2 instance via SSH.

**Note:**<br/>
This is not the node where Jenkins is running. This is the intermediate EC2 (Bastion Host) used for accessing private resources like your EKS cluster.

**8. Configure AWS CLI on Bastion Server**
Run the AWS configure command:<br/>
```bash
aws configure
```
Add your Access Key and Secret Key when prompted.

**9. Update Kubeconfig for EKS**<br/>
Run the following important command:
```bash
aws eks update-kubeconfig --region ap-southeast-2 --name tws-eks-cluster
```
* This command maps your EKS cluster with your Bastion server.
* It helps to communicate with EKS components.

**10. Install AWS application load balancer refering the below docs link**<br/>
```
https://docs.aws.amazon.com/eks/latest/userguide/lbc-helm.html


```

prerequsits,å‰ææ˜¯éœ€è¦å®‰è£…å¾ˆå¤š cnis ä¹‹ç±»çš„
å¯ä»¥ä½¿ç”¨çœ‹ k get pods -n system
the controller watches for Kubernetes Ingress or Service resources. In response, it creates the appropriate AWS Elastic Load Balancing resources

he controller provisions the following resources:

  Kubernetes Ingress
  The LBC creates an AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress. Review the annotations you can apply to an Ingress resource.

  Kubernetes service of the LoadBalancer type
  The LBC creates an AWS Network Load Balancer (NLB)when you create a Kubernetes service of type LoadBalancer. Review the annotations you can apply to a Service resource.
å¦‚æœä½¿ç”¨çš„æ˜¯æœåŠ¡è´¦æˆ·çš„ IAM è§’è‰²ï¼ˆIRSAï¼‰ï¼Œåˆ™å¿…é¡»ä¸ºæ¯ä¸ªé›†ç¾¤è®¾ç½® IRSAï¼Œå¹¶ä¸”è§’è‰²ä¿¡ä»»ç­–ç•¥ä¸­çš„ OpenID Connectï¼ˆOIDCï¼‰æä¾›è€… ARN åº”ç‰¹å®šäºæ¯ä¸ª EKS é›†ç¾¤


eksctl create iamserviceaccount \
    --cluster=tws-eks-cluster \
    --namespace=kube-system \
    --name=aws-load-balancer-controller \
    --attach-policy-arn=arn:aws:iam::277707141977:policy/AWSLoadBalancerControllerIAMPolicy \
    --override-existing-serviceaccounts \
    --region ap-southeast-2 \
    --approve

    ![alt text](image-18.png)
    implemeting the cloudformation to create the sa.role. ,å» conslole validateï¼Œ éªŒè¯ä¸€ä¸‹ã€‚ cloudformationã€‚
    k get sa  aws-load-balncer-controler -n kube-system
    ç„¶åstep 2 
é€šè¿‡helm æ¥å®‰è£…alb controller
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=tws-eks-cluster \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set region=ap-southeast-2
  --set vpcId=vpc-0f29bb76897c77a96
  --version 1.13.0
åº”è¯¥èƒ½çœ‹åˆ° alb controller 
k  get sa -n kube-system 
k get deploymetn -n kube-system aws-load-balancer.

æ³¨æ„çš„æ˜¯ The helm install command automatically installs the custom resource definitions (CRDs) for the controller. The helm upgrade command does no
kubectl get deployment -n kube-system aws-load-balancer-controller å‘½ä»¤
**11. Install the EBS CSI driver refering the below docs link**<br/>
same process as alb controler,  æ¯”è¾ƒç®€å•çš„æ˜¯ä½¿ç”¨ eksctl æ¥å®‰è£…ã€‚  ç„¶åå»çœ‹cloudformation.
```
https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html#eksctl_store_app_data
```
Amazon EBS volumes and the Amazon EBS CSI driver are not compatible with Amazon EKS Hybrid Nodes.

The Amazon EBS CSI plugin requires IAM permissions to make calls to AWS APIs on your behalf.

å¦‚æœä½¿ç”¨kmsçš„è¯åœ¨ cis  å®‰è£…çš„policyä¸­æ·»åŠ ã€‚u do not use a custom KMS key. If y

1  æ–°å»ºä¸€ä¸ª iam role

å’Œ  alb ç±»ä¼¼ã€‚  ç„¶åå» helm  install csi driver
eksctl create iamserviceaccount \
        --name ebs-csi-controller-sa \
        --namespace kube-system \
        --cluster tws-eks-cluster \
        --role-name AmazonEKS_EBS_CSI_DriverRole \
        --role-only \
        --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
        --approve

ä¸é€‚ç”¨ä¸‹é¢çš„å¦‚æœæ²¡æœ‰kms
<!-- aws iam create-policy \
      --policy-name KMS_Key_For_Encryption_On_EBS_Policy \
      --policy-document file://kms-key-for-encryption-on-ebs.json

Attach the IAM policy to the role with the following command.
aws iam attach-role-policy \
      --policy-arn arn:aws:iam::111122223333:policy/KMS_Key_For_Encryption_On_EBS_Policy \
      --role-name AmazonEKS_EBS_CSI_DriverRole -->

step 2  use helm to depoy the driver . ä¹Ÿå¯ä»¥ä½¿ç”¨kustomize 
helm upgrade --install aws-ebs-csi-driver \
    --namespace kube-system \
    aws-ebs-csi-driver/aws-ebs-csi-driver
éœ€è¦ k  get pods -n kube-system -l app.kubernetes.io/name=aws-cis-driver  w æ¥çœ‹ä¿¡æ¯



**12. Argo CD Setup**<br/>
Create a Namespace for Argo CD<br/>
```bash
kubectl create namespace argocd
```
1. Install Argo CD using helm  
(https://artifacthub.io/packages/helm/argo/argo-cd)


```bash
helm repo add argo https://argoproj.github.io/argo-helm
helm install my-argo-cd argo/argo-cd --version 8.2.7 -n argocd11
``
1ï¼Œ ä¸ºäº†access argocdï¼Œä¸€ä¸ªæ˜¯å»helm repo check the default values. 
ä¸€ä¸ªæ˜¯ä½¿ç”¨ ingress æˆ–è€… loadballncer æ‰€ä»¥æ‰éœ€è¦é…ç½® values
2. get the values file and save it ******
ä¸ºä»€ä¹ˆæ˜¯è¿™ä¸ªå‘½ä»¤ helm show values argo/argo-cd ä¼šæ˜¾ç¤º argo/argo-cd è¿™ä¸ª Helm Chart çš„æ‰€æœ‰å¯é…ç½®å‚æ•°
```bash
helm show values argo/argo-cd > argocd-values.yaml  
```
3. edit the values file, change the below settings.   ä¸ºäº†foring exposing the argocd in ingress.

åœ¨ artifcat hub é¡µé¢ä¸­å¯ä»¥æŸ¥çœ‹ å¦‚ä½•é…ç½® alb ingress
![alt text](image-28.png)
```
global:
  domain: argocd.example.com  æœç´¢  ingress  å‰ææ˜¯æ£€æµ‹ argocd running or notã€‚ k get po -n argocd  ç„¶åvi  set number 

configs:
  params:
    server.insecure: true
è¿™é‡Œå¼€å¯argo cdçš„ ingress ä» argo cd å®˜ç½‘çš„ aws loadbalancerä¸­æ‹·è´ã€‚ 
https://argo-cd.readthedocs.io/en/stable/operat-or-manual/ingress/#aws-application-load-balancers-albs-and-classic-elb-http-mode
server:
  ingress:
    enabled: true # è¿™é‡Œå¼€å¯argo cdçš„ ingress ä» argo cd å®˜ç½‘çš„ aws loadbalancerä¸­æ‹·è´ã€‚
    controller: aws  # éƒ½æ˜¯é»˜è®¤å€¼ã€‚
    ingressClassName: alb  # ingressClassName
    annotations:
      alb.ingress.kubernetes.io/scheme: internet-facing 
      alb.ingress.kubernetes.io/certificate-arn: <your-cert-arn>  #ssl/tls certificat
      alb.ingress.kubernetes.io/group.name: easyshop-app-lb  # combine all the ingress resoure åˆ°ä¸€ä¸ª groupä¸­ï¼Œä¸éœ€è¦åˆ›å»ºå¤šä¸ª alb
      alb.ingress.kubernetes.io/target-type: ip æˆ–è€… instance #ä¸é€‚ç”¨nodeportï¼Œéœ€è¦ä¿®æ”¹wei clster ip ä¹Ÿå°±æ˜¯ip   ä¼šæŠŠtfraffic  forward åˆ°ä¸€ä¸ª target group. ä¹Ÿå°±æ˜¯ POD  ip ä¹Ÿå°±æ˜¯ targetgroupé‡Œé¢çš„pod çš„IP.  traget-type æ˜¯ ip addressã€‚ ä¸æ˜¯instance ip.
      alb.ingress.kubernetes.io/backend-protocol: HTTP
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}, {"HTTPS":443}]'
      alb.ingress.kubernetes.io/ssl-redirect: '443'  if the traffic from http, it will be redirect to https 
    hostname: argocd.devopsdock.site   # hostname å°±æ˜¯ä¸€ä¸ªsubdomomin
    aws:
      serviceType: ClusterIP # <- Used with target-type: ip  
      backendProtocolVersion: GRPC
æœç´¢  alb ingress annataions èƒ½çœ‹åˆ°ä¸‹é¢çš„ å…³äº group
Security Risk

IngressGroup feature should only be used when all Kubernetes users with RBAC permission to create/modify Ingress resources are within trust boundary.

If you turn your Ingress to belong a "explicit IngressGroup" by adding group.name annotation, other Kubernetes user may create/modify their Ingresses to belong same IngressGroup, thus can add more rules or overwrite existing rules with higher priority to the ALB for your Ingress.

We'll add more fine-grained access-control in future versions. 

    ä½¿ç”¨vim  æŠ€å·§   set number å’Œ /æ¥æœç´¢ã€‚ alias k=kubectl 
    
```
4. save and upgrade the helm chart.
```
helm upgrade my-argo-cd argo/argo-cd -n argocd -f my-values.yaml

k get ingress argocd
è¿™æ—¶å€™ä¼šåˆ›å»ºä¸€ä¸ª loadbalancer.
listern rulesï¼Œ ä¼šæŠŠtfraffic  forward åˆ°ä¸€ä¸ª target group. ä¹Ÿå°±æ˜¯ port iip.  traget-type æ˜¯ ip addressã€‚ ä¸æ˜¯instance ip.
k get po -n argocd -o wideï¼Œ
æˆ‘ä»¬èƒ½çœ‹åˆ° è¿™é‡Œé¢çš„ argocd server pod ip å°±æ˜¯å‰é¢çš„ target ips.
 ä¸€ä¸ªæ˜¯grpc ä¸€ä¸ªæ˜¯ http çœ‹è¿™é‡Œçš„listener ruleã€‚ è¿™ä¸ªrule å°±æ˜¯
 ![alt text](image-19.png)


è¿™é‡Œé¢çš„ target group å°±æ˜¯  pod  argocd server çš„ip
![alt text](image-20.png)
```
5. add the record in route53 â€œargocd.devopsdock.siteâ€ with load balancer dns.

record type ä½¿ç”¨a reocrdï¼Œ ç„¶å  alias ã€‚ã€‚ é€‰æ‹© loadbalancer":  a rcord alias


A è®°å½•ï¼ˆAddress Recordï¼‰

ä½œç”¨ï¼šå°†åŸŸåç›´æ¥è§£æåˆ°ä¸€ä¸ª IPv4 åœ°å€ã€‚
åœºæ™¯ï¼šå°† example.com è§£æåˆ° EC2ã€ELBã€ALB çš„å…¬ç½‘ IPã€‚
ç¤ºä¾‹ï¼šeasyshop.devopsdock.site â†’ 1.2.3.4
CNAME è®°å½•ï¼ˆCanonical Name Recordï¼‰

ä½œç”¨ï¼šå°†ä¸€ä¸ªåŸŸåæŒ‡å‘å¦ä¸€ä¸ªåŸŸåï¼ˆä¸èƒ½ç”¨äºæ ¹åŸŸåï¼‰ã€‚
åœºæ™¯ï¼šwww.example.com â†’ example.comï¼Œæˆ–æŒ‡å‘ CloudFront/ELB DNS åç§°ã€‚
ç¤ºä¾‹ï¼šwww.easyshop.devopsdock.site â†’ easyshop.devopsdock.site
Alias è®°å½•ï¼ˆAWS ç‰¹æœ‰ï¼‰

ä½œç”¨ï¼šå°†åŸŸåç›´æ¥æŒ‡å‘ AWS èµ„æºï¼ˆå¦‚ ALBã€CloudFrontã€S3 é™æ€ç½‘ç«™ï¼‰ï¼Œä¸æ¶ˆè€— DNS æŸ¥è¯¢æ¬¡æ•°ã€‚
åœºæ™¯ï¼šæ ¹åŸŸåæŒ‡å‘ ALB/CloudFrontã€‚
ç¤ºä¾‹ï¼šeasyshop.devopsdock.site â†’ my-alb-1234567890.us-west-2.elb.amazonaws.com

6. access it in browser.

7. Retrive the secret for Argocd

```jsx
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
```

8. login to argocd â€œadminâ€ and retrieved password

9. Change the password by going to â€œuser infoâ€ tab in the UI.

**Deploy Your Application in Argo CD GUI**

> On the Argo CD homepage, click on the â€œNew Appâ€ button.
>

> Fill in the following details:
> 
> - **Application Name:**Â `Enter your desired app name`
> - **Project Name:**Â SelectÂ `default`Â from the dropdown.
> - **Sync Policy:**Â ChooseÂ `Automatic`.

> In theÂ SourceÂ section:
> 
> - **Repo URL:**Â Add the Git repository URL that contains your Kubernetes manifests. 
> - **Path:**Â `Kubernetes`Â (or the actual path inside the repo where your manifests reside) ä¸»è¦æ˜¯manifest files
argo cd ä¼šè‡ªåŠ¨pick up the manifest file of the k8s
> In the â€œDestinationâ€ section:
> 
> - **Cluster URL:**Â [https://kubernetes.default.svc](https://kubernetes.default.svc/)Â (usually shown as "default") 
> - **Namespace:**Â tws-e-commerce-app (or your desired namespace)
 ç„¶åå°±èƒ½çœ‹åˆ°
> Click on â€œCreateâ€.
> argo CD çš„ â€œNew Appâ€ æ˜¯æŒ‡åœ¨ Argo CD ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„åº”ç”¨ï¼ˆApplicationï¼‰å¯¹è±¡ã€‚å®ƒçš„æ ¸å¿ƒæ¦‚å¿µå¦‚ä¸‹ï¼š

Application æ˜¯ Argo CD çš„æ ¸å¿ƒèµ„æºï¼Œä»£è¡¨ä½ è¦åœ¨ Kubernetes é›†ç¾¤ä¸­æŒç»­éƒ¨ç½²å’Œç®¡ç†çš„ä¸€ä¸ªåº”ç”¨ã€‚
æ¯ä¸ª Application ä¼šç»‘å®šä¸€ä¸ª Git ä»“åº“ï¼ˆæˆ– Helm ä»“åº“ï¼‰å’Œä¸€ä¸ªç›®æ ‡é›†ç¾¤/å‘½åç©ºé—´ï¼ŒArgo CD ä¼šè‡ªåŠ¨åŒæ­¥ä»“åº“ä¸­çš„ K8s é…ç½®åˆ°é›†ç¾¤ã€‚
ä¸»è¦é…ç½®é¡¹ï¼š

Application Nameï¼šåº”ç”¨çš„å”¯ä¸€åç§°ã€‚
Projectï¼šå½’å±çš„ Argo CD é¡¹ç›®ï¼ˆå¯ç”¨äºæƒé™éš”ç¦»ï¼‰ã€‚
Sync Policyï¼šåŒæ­¥ç­–ç•¥ï¼ˆæ‰‹åŠ¨/è‡ªåŠ¨ï¼‰ã€‚
Sourceï¼šä»£ç ä»“åº“åœ°å€ã€åˆ†æ”¯ã€è·¯å¾„ï¼ˆK8s manifests/Helm chart æ‰€åœ¨ç›®å½•ï¼‰ã€‚
Destinationï¼šç›®æ ‡é›†ç¾¤å’Œå‘½åç©ºé—´ã€‚
Revisionï¼šè¦éƒ¨ç½²çš„ Git åˆ†æ”¯ã€tag æˆ– commitã€‚
ä½œç”¨ï¼š

è®© GitOps è‡ªåŠ¨åŒ–è½åœ°ï¼Œä»£ç å˜æ›´è‡ªåŠ¨è§¦å‘é›†ç¾¤å˜æ›´ã€‚
å¯è§†åŒ–ç®¡ç†ã€å›æ»šã€å¯¹æ¯”ã€å®¡è®¡åº”ç”¨çš„æ‰€æœ‰å˜æ›´ã€‚
ç®€å•è¯´ï¼ŒNew App å°±æ˜¯æŠŠä½ çš„ Git ä»“åº“å’Œ K8s é›†ç¾¤â€œç»‘å®šâ€èµ·æ¥ï¼Œå®ç°æŒç»­éƒ¨ç½²å’Œå£°æ˜å¼ç®¡ç†ã€‚

NOTE: before deploying Chnage your ingress settings and image tag in the yamls inside â€œkubernetesâ€ directory

Ingress Annotations: 

```jsx
annotations:
    alb.ingress.kubernetes.io/group.name: easyshop-app-lb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-southeast-2:876997124628:certificate/b69bb6e7-cbd1-490b-b765-27574080f48c
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/backend-protocol: HTTP
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}, {"HTTPS":443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'
    kubernetes.io/ingress.class: alb
```

- **add record to route 53 â€œeasyshop.devopsdock.siteâ€** è¿™æ¬¡å¯ä»¥ä½¿ç”¨cname.

- **Access your site now.**

### Install Metric Server

- metric server install thru helm chart
 is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines.
```
https://artifacthub.io/packages/helm/metrics-server/metrics-server
```
verify metric server.
```
kubectl get pods -w
kubectl top pods èƒ½çœ‹åˆ° metrics cupu of the node.
```
### Monitoring Using kube-prometheus-stack è¿™æ˜¯ä¸€ä¸ª operatorè¿˜æ˜¯ helm?
de easy to operate end-to-end Kubernetes cluster monitoring with  using the . 
é‡Œé¢æœ‰inbuild prometheus rules . 
kube-prometheus-stack æ—¢æ˜¯ä¸€ä¸ª Helm Chartï¼Œä¹Ÿæ˜¯ä¸€ä¸ª Operator æ–¹æ¡ˆã€‚å®ƒé€šè¿‡ Helm Chart å®‰è£…ï¼Œä½†æ ¸å¿ƒç»„ä»¶ï¼ˆå¦‚ Prometheus Operatorï¼‰è´Ÿè´£è‡ªåŠ¨åŒ–ç®¡ç†å’Œè‡ªæ„ˆ Prometheusã€Alertmanagerã€Grafana ç­‰ç›‘æ§èµ„æºã€‚

åŒºåˆ«å¦‚ä¸‹ï¼š

ä¼ ç»Ÿ Helm Chartï¼ˆå¦‚å•ç‹¬çš„ prometheusã€grafanaï¼‰åªæ˜¯ç®€å•éƒ¨ç½²åº”ç”¨ï¼Œåç»­èµ„æºï¼ˆå¦‚ ServiceMonitorã€Ruleã€Alertï¼‰éœ€è¦æ‰‹åŠ¨ç®¡ç†ã€‚
kube-prometheus-stack é€šè¿‡ Helm å®‰è£… Prometheus Operatorï¼ŒOperator ä¼šè‡ªåŠ¨å‘ç°ã€ç®¡ç†å’Œè‡ªæ„ˆç›‘æ§ç›¸å…³çš„è‡ªå®šä¹‰èµ„æºï¼ˆå¦‚ ServiceMonitorã€PrometheusRuleï¼‰ï¼Œå®ç°å£°æ˜å¼å’Œè‡ªåŠ¨åŒ–è¿ç»´ã€‚
create a namespace â€œmonitoringâ€

```jsx
kubectl create ns monitoring
```
```
https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack
```
verify deployment :

```jsx
kubectl get pods -n monitoring
```

get the helm values and save it in a file

```jsx  è¿™ä¸ªå¾ˆé‡è¦
helm show values prometheus-community/kube-prometheus-stack > kube-prom-stack.yaml 
```

edit the file and add the following in the params for prometheus, grafana and alert manger.
ä¿®æ”¹æ¯ä¸€ä¸ªçš„ingress part.  
éœ€è¦æœç´¢default values åœ¨ helm install è§£å¯†ä¸­ æœç´¢  alertmanger.ingress
**Grafana:**

```jsx
ingressClassName: alb  # k get ingressclass. controle name æ˜¯albå½“æˆ‘ä»¬å®‰è£…loagbalncer controllerçš„æ—¶å€™ä¼šè‡ªåŠ¨å®‰è£… ingress class name
annotations:
      alb.ingress.kubernetes.io/group.name: easyshop-app-lb
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-southeast-2:876997124628:certificate/b69bb6e7-cbd1-490b-b765-27574080f48c
      alb.ingress.kubernetes.io/target-type: ip
			alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}, {"HTTPS":443}]'
      alb.ingress.kubernetes.io/ssl-redirect: '443'
 
    hosts:
      - grafana.devopsdock.site
```

**Prometheus:** 
å¯ä»¥ç›´æ¥/ prometheus.domain.name
```jsx
ingressClassName: alb
annotations:
      alb.ingress.kubernetes.io/group.name: easyshop-app-lb
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-southeast-2:876997124628:certificate/b69bb6e7-cbd1-490b-b765-27574080f48c
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}, {"HTTPS":443}]'
      alb.ingress.kubernetes.io/ssl-redirect: '443'
    labels: {}

    ç„¶åè¦ç»™ host name
  
    hosts: 
      - prometheus.devopsdock.site
        paths:
        - /
        pathType: Prefix  path typï¼š ä¸è¦å¿˜è®°è¿™ä¸ªæ˜¯ Prefix   ä¹Ÿå¯ä»¥æ˜¯exact?


```
åœ¨ Kubernetes Ingress é‡Œï¼ŒpathType æœ‰ä¸‰ç§å¸¸ç”¨ç±»å‹ï¼šPrefixã€Exactã€å’Œ ImplementationSpecificã€‚ä½ é—®çš„ Prefix å’Œ Exact åŒºåˆ«å¦‚ä¸‹ï¼š

pathType: Prefix

åŒ¹é…æ‰€æœ‰ä»¥æŒ‡å®šè·¯å¾„å¼€å¤´çš„è¯·æ±‚ã€‚ä¾‹å¦‚ï¼Œpath: / ç»“åˆ Prefixï¼Œä¼šåŒ¹é… /ã€/fooã€/bar/baz ç­‰æ‰€æœ‰è·¯å¾„ã€‚
é€‚åˆå¤§å¤šæ•° Web åº”ç”¨ï¼Œå°¤å…¶æ˜¯ä½ æƒ³è®©æ‰€æœ‰å­è·¯å¾„éƒ½è·¯ç”±åˆ°åŒä¸€ä¸ªæœåŠ¡æ—¶ã€‚
ä¾‹å­ï¼špath: /apiï¼ŒpathType: Prefixï¼Œä¼šåŒ¹é… /apiã€/api/v1ã€/api/foo ç­‰ã€‚
**Alertmanger:**
```jsx
ingressClassName: alb
annotations:
      alb.ingress.kubernetes.io/group.name: easyshop-app-lb
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/backend-protocol: HTTP
			alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}, {"HTTPS":443}]'
      alb.ingress.kubernetes.io/ssl-redirect: '443'
    
    hosts: 
      - alertmanager.devopsdock.site
    paths:
    - /
    pathType: Prefix
```


helm upgrade my-kube-prometheus-stack prometheus-community/kube-prometheus-stack -f kube-prom-stack.yaml -n monitoring
xéœ€è¦ upgrade   yml file.
k get ingress -n moitoring.
èƒ½çœ‹åˆ° ä¸‰ä¸ª ã€‚ ingress class ä¸º albçš„  ingress.
grafana, prometheus, alert manager.
![alt text](image-1.png)
èƒ½çœ‹åˆ°æ›´æ–°çš„rule,ç„¶åå†å» route 53. add the record æ‰€æœ‰çš„hostname CNAME åˆ° alb çš„value
![alt text](image-2.png)


èƒ½çœ‹åˆ°  mdashboardçš„ é»˜è®¤å€¼ã€‚ grafana - dashboard.  é»˜è®¤çš„ built-inçš„rulesåŒ…å«å…¶ä»–çš„ã€‚
![alt text](image-4.png)
![alt text](image-3.png)
modify the helm values.
**Alerting to Slack** 
![alt text](image.png)
Create a new workspace in slack, create a new channel e.g. â€œ#alertsâ€ åˆ›å»ºä¸€ä¸ªæ–°çš„workspaceå’Œ new channel

go to https://api.slack.com/apps to create the webhook.
create a new app ç±»ä¼¼ azure ä¸­çš„  apps ã€‚ã€‚
![alt text](image-5.png)
1. create an app â€œalertmanagerâ€
2. go to incoming webhook
3. create a webhook and copy it.  chouse the channle ä½ æƒ³postçš„
æ‹·è´è¿™ä¸ªwebhook
![alt text](image-6.png)

åˆå¾—å» alartmanager confg in the  yml.å¾—åˆ°çš„ç»“æœå¦‚ä¸‹
![alt text](image-7.png)
```jsx
config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['namespace']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'slack-notification'
      routes:
      - receiver: 'slack-notification'
        matchers:
          - severity = "critical" # è¿™ä¸ªæ˜¯åœ¨ alert yaml ruleä¸­å®šä¹‰çš„ã€‚ æœ‰example rules.
    receivers:
    - name: 'slack-notification'
      slack_configs:
          - api_url: 'https://hooks.slack.com/services/T08ULBZB5UY/B08U0CE3DEG/OivCLYq28gNzz4TabiY5zUj'
            channel: '#alerts'
            send_resolved: true
    templates:
    - '/etc/alertmanager/config/*.tmpl'
```
route: å‘Šè­¦è·¯ç”±è§„åˆ™ï¼Œå†³å®šå‘Šè­¦å¦‚ä½•åˆ†ç»„ã€å»¶è¿Ÿã€å‘é€åˆ°å“ªä¸ª receiver
group_by: æŒ‰å“ªäº›æ ‡ç­¾åˆ†ç»„
group_wait: ç¬¬ä¸€æ¬¡åˆ†ç»„ç­‰å¾…æ—¶é—´
group_interval: åŒç»„å‘Šè­¦å†æ¬¡å‘é€çš„é—´éš”
repeat_interval: é‡å¤å‘é€çš„é—´éš”
receiver: é»˜è®¤æ¥æ”¶å™¨
routes: å­è·¯ç”±ï¼ŒæŒ‰æ¡ä»¶ï¼ˆå¦‚ severityï¼‰è¿›ä¸€æ­¥åˆ†æµ
receivers: å‘Šè­¦æ¥æ”¶å™¨é…ç½®ï¼Œè¿™é‡Œé…ç½®äº† Slack é€šçŸ¥
slack_configs: Slack Webhook åœ°å€ã€é¢‘é“ã€æ˜¯å¦å‘é€å·²æ¢å¤é€šçŸ¥ç­‰
templates: è‡ªå®šä¹‰å‘Šè­¦æ¶ˆæ¯æ¨¡æ¿è·¯å¾„
Note: you can refer this DOCs for the slack configuration. â€œhttps://prometheus.io/docs/alerting/latest/configuration/#slack_configâ€ 

upgrade the chart

```jsx
helm upgrade my-kube-prometheus-stack prometheus-community/kube-prometheus-stack -f kube-prom-stack.yaml -n monitoring
```

get grafana secret â€œuser = adminâ€

```jsx
kubectl --namespace monitoring get secrets my-kube-prometheus-stack-grafana -o jsonpath="{.data.admin-password}" | base64 -d ; echo
```

You would get the notification in the slackâ€™s respective channel.

## **Logging**

- we will use elasticsearch for logsstore, filebeat for log shipping and kibana for the visualization.
æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª persisit volumeï¼Œ t

```
NOTE: The EBS driver we installed is for elasticsearch to dynamically provision an EBS volume.
```
**Install Elastic Search:**

```jsx
helm repo add elastic https://helm.elastic.co -n logging
helm install my-elasticsearch elastic/elasticsearch --version 8.5.1 -n logging
```
k  get po -n lggging ä¼šå‘ç°æœ‰pending 
![alt text](image-8.png)
lets search  default vlause    volumClaimTemplate.   éœ€è¦talk to storage class.

****é‡è¦Create a storageclass so that elastic search can dynamically provision volume in AWS.

storageclass.yaml

```jsx
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-aws
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: ebs.csi.aws.com
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
```

apply the yaml file.

get the values for elastic search helm chart.

```jsx
helm show values elastic/elasticsearch > elasticsearch.yaml 
```

update the values

```jsx
replicas: 1  # ä¸€èˆ¬ 1ä¸ª master  2ä¸ª  data.  minimumasternode:1  replicas: 1éƒ½è®¾ç½®ä¸º1
minimumMasterNodes: 1
clusterHealthCheckParams: "wait_for_status=yellow&timeout=1s"
```

è¿™äº›è®¾ç½®æ˜¯ä¸ºäº†è®© Elasticsearch åœ¨å•èŠ‚ç‚¹ï¼ˆå¼€å‘/æµ‹è¯•ç¯å¢ƒï¼‰ä¸‹èƒ½æ­£å¸¸å¯åŠ¨å’Œå·¥ä½œï¼š

replicas: 1ï¼šåªå¯åŠ¨ä¸€ä¸ª Elasticsearch å®ä¾‹ï¼ˆPodï¼‰ï¼Œå³å•èŠ‚ç‚¹æ¨¡å¼ã€‚
minimumMasterNodes: 1ï¼šä¸»èŠ‚ç‚¹æ•°é‡è®¾ä¸º 1ï¼Œé¿å…é›†ç¾¤å¯åŠ¨æ—¶å› ç¼ºå°‘ä¸»èŠ‚ç‚¹è€ŒæŠ¥é”™ã€‚
clusterHealthCheckParams: "wait_for_status=yellow&timeout=1s"ï¼š
yellow çŠ¶æ€è¡¨ç¤ºä¸»åˆ†ç‰‡åˆ†é…å®Œæˆï¼Œä½†å‰¯æœ¬åˆ†ç‰‡æœªåˆ†é…ï¼ˆå•èŠ‚ç‚¹æ— æ³•åˆ†é…å‰¯æœ¬ï¼‰ã€‚
åœ¨å•èŠ‚ç‚¹æƒ…å†µä¸‹ï¼Œé›†ç¾¤å¥åº·åªèƒ½è¾¾åˆ° yellowï¼ˆä¸æ˜¯ greenï¼‰ï¼Œä½†å¯ä»¥æ­£å¸¸è¯»å†™æ•°æ®ã€‚
è¿™æ ·è®¾ç½®å¯ä»¥è®© Helm/Operator æ£€æŸ¥åˆ° yellow å°±è®¤ä¸ºé›†ç¾¤å¥åº·ï¼Œé¿å…å› å‰¯æœ¬æœªåˆ†é…å¯¼è‡´å®‰è£…/å‡çº§å¤±è´¥ã€‚
å¦‚æœä½ æ˜¯ç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®è‡³å°‘ 3 ä¸ªèŠ‚ç‚¹ï¼ŒminimumMasterNodes è®¾ç½®ä¸º 2 æˆ– 3ï¼Œå¥åº·æ£€æŸ¥è¦æ±‚ greenã€‚ä½†å¼€å‘/æµ‹è¯•ç¯å¢ƒå•èŠ‚ç‚¹åªèƒ½æ˜¯ yellowã€‚

upgrade the chart

```jsx
helm upgrade my-elasticsearch elastic/elasticsearch -f elasticsearch.yaml -n logging
```

if upgarde doesnt happen then uninstall and install it again.

make sure the pod is running .

```jsx
kubectl get po -n logging
NAME                     READY   STATUS    RESTARTS   AGE
elastic-operator-0       1/1     Running   0          6h33m
elasticsearch-master-0   1/1     Running   0          87m
```
k  get statefulset -n logging   æœ‰ä¸€ä¸ªmaster
![alt text](image-10.png)
**FileBeat:**

Elasticsearch éœ€è¦æŒä¹…åŒ–å­˜å‚¨ï¼ˆæ•°æ®ä¸èƒ½å›  Pod é‡å¯è€Œä¸¢å¤±ï¼‰ï¼ŒStatefulSet æ”¯æŒæŒ‚è½½ PersistentVolumeã€‚
æ¯ä¸ª Elasticsearch èŠ‚ç‚¹ï¼ˆPodï¼‰æœ‰å”¯ä¸€çš„åå­—å’Œèº«ä»½ï¼ˆå¦‚ elasticsearch-master-0ï¼‰ï¼Œæ–¹ä¾¿é›†ç¾¤å‘ç°å’Œä¸»ä»é€‰ä¸¾ã€‚
StatefulSet èƒ½ä¿è¯ Pod é¡ºåºå¯åŠ¨ã€ç¨³å®šçš„ç½‘ç»œæ ‡è¯†ï¼ˆDNSï¼‰ï¼Œé€‚åˆåˆ†å¸ƒå¼æ•°æ®åº“ã€‚
è¿™é‡Œ éœ€è¦check  pv  pvc  

![alt text](image-11.png)
å› ä¸º statefulset to PVC will not delete, we have to delete it manually.
k  delete pvc  e-master-e-master-0     master-1 master-2


åœ¨ Kubernetes é‡Œï¼ŒStatefulSet ä¼šä¸ºæ¯ä¸ª Pod è‡ªåŠ¨åˆ›å»ºä¸€ä¸ª PersistentVolumeClaimï¼ˆPVCï¼‰ï¼Œç”¨äºæŒ‚è½½æŒä¹…åŒ–å­˜å‚¨ï¼ˆPVï¼‰ã€‚æ¯”å¦‚ elasticsearch-master-0 ä¼šæœ‰ä¸€ä¸ªåä¸º elasticsearch-master-elasticsearch-master-0 çš„ PVCã€‚

å½“ä½ åˆ é™¤ StatefulSet æˆ– Pod æ—¶ï¼Œè¿™äº› PVC ä¸ä¼šè‡ªåŠ¨åˆ é™¤ï¼ˆé˜²æ­¢è¯¯åˆ æ•°æ®ï¼‰ï¼Œéœ€è¦ä½ æ‰‹åŠ¨åˆ é™¤ã€‚
kubectl delete pvc elasticsearch-master-elasticsearch-master-0 å°±æ˜¯æ‰‹åŠ¨åˆ é™¤è¿™ä¸ª PVCã€‚
å¦‚æœä½ æœ‰å¤šä¸ªèŠ‚ç‚¹ï¼ˆå¦‚ master-1ã€master-2ï¼‰ï¼Œä¹Ÿè¦åˆ†åˆ«åˆ é™¤å¯¹åº”çš„ PVCã€‚
PVC pending é€šå¸¸æ˜¯å› ä¸ºæ²¡æœ‰åˆé€‚çš„ StorageClass æˆ– PV å¯ç”¨ã€‚æ‰‹åŠ¨åˆ é™¤ PVC å¯ä»¥é‡Šæ”¾å­˜å‚¨èµ„æºï¼Œé¿å…é—ç•™æ— ç”¨çš„ç£ç›˜ã€‚


éœ€è¦å»çœ‹ æœ€æ—©çš„ csi driver ã€‚ ã€‚
   

   ![alt text](image-12.png)
   æ²¡æœ‰role

![alt text](image-13.png)

uninstall and  install it again!!!!!!!!!!
![alt text](image-14.png)

ç„¶å grep  role 
![alt text](image-15.png)

ç„¶åèƒ½çœ‹åˆ°è¿™ä¸ªvolume 
![alt text](image-16.png)
install filebeat for log shipping.

```jsx
helm repo add elastic https://helm.elastic.co
helm install my-filebeat elastic/filebeat --version 8.5.1 -n logging
```

get the values

```jsx
helm show values elastic/filebeat > filebeat.yaml 
```

Filebeat runs as a daemonset. check if its up.

```jsx
filebeat ä¸€èˆ¬æ˜¯ dameaon setï¼Œ 
kubectl get po -n logging
NAME                         READY   STATUS    RESTARTS   AGE
elastic-operator-0           1/1     Running   0          6h38m
elasticsearch-master-0       1/1     Running   0          93m
my-filebeat-filebeat-g79qs   1/1     Running   0          25s
my-filebeat-filebeat-kh8mj   1/1     Running   0          25s
```

**Install Kibana:**

install kibana through helm.

```jsx
helm repo add elastic https://helm.elastic.co
helm install my-kibana elastic/kibana --version 8.5.1 -n logging
```

Verify if it runs.

```jsx
k get po -n logging
NAME                               READY   STATUS    RESTARTS       AGE
elastic-operator-0                 1/1     Running   0              8h
elasticsearch-master-0             1/1     Running   0              3h50m
my-filebeat-filebeat-g79qs         1/1     Running   0              138m
my-filebeat-filebeat-jz42x         1/1     Running   0              108m
my-filebeat-filebeat-kh8mj         1/1     Running   1 (137m ago)   138m
my-kibana-kibana-559f75574-9s4xk   1/1     Running   0              130m
```

get values

```jsx
helm show values elastic/kibana > kibana.yaml 
```

modify the values for ingress settings

```jsx
ingress:
  enabled: true
  className: "alb"
  pathtype: Prefix
  annotations:
    alb.ingress.kubernetes.io/group.name: easyshop-app-lb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/backend-protocol: HTTP
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-southeast-2:876997124628:certificate/b69bb6e7-cbd1-490b-b765-27574080f48c
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}, {"HTTPS":443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'
  # kubernetes.io/ingress.class: nginx
  # kubernetes.io/tls-acme: "true"
  hosts:
    - host: logs-kibana.devopsdock.site
      paths:
        - path: /
```

save the file and exit. upgrade the helm chart using the values file.

```jsx
helm upgrade my-kibana elastic/kibana -f kibana.yaml -n logging
```

add all the records to route 53 and give the value as load balancer DNS name. and try to access one by one. 
 
retrive the secret of elastic search as kibanaâ€™s password, username is â€œelasticâ€

```jsx
kubectl get secrets --namespace=logging elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 -d
```
![alt text](image-17.png)

èƒ½çœ‹åˆ° æ‰€æœ‰conatiner çš„  stream ã€‚
### **Filebeat Configuration to ship the "easyshop" app logs to elasticsearch**

configure filebeat to ship the application logs to view in kibana

```jsx
filebeatConfig:
    filebeat.yml: |
      filebeat.inputs:
      - type: container
        paths:
          - /var/log/containers/*easyshop*.log  åŸæ¥æ˜¯ *.log
          instaead of fecting all the logs, just  fetch the easyshop logï¼Œ using the wild card é€šé…ç¬¦ã€‚
```

upgrade filebeat helm chart and check in kibanaâ€™s UI if the app logs are streaming.

## **Congratulations!** <br/>
![EasyShop Website Screenshot](./public/easyshop.JPG)

---

### ğŸ“Œ Architecture Diagram
![Diagram](./public/diagram-export.JPG)

---

### ğŸ“Œ ArgoCD
![ArgoCD](./public/Argocd.JPG)

---

### ğŸ“Œ Capture
![Capture](./public/Capture.JPG)

---

### ğŸ“Œ AlertManager
![AlertManager](./public/alertManager.JPG)


---

### ğŸ“Œ Grafana Dashboard
![Grafana](./public/grafana.JPG)

---

### ğŸ“Œ Kibana Logs View
![Kibana](./public/kibana.JPG)

---

### ğŸ“Œ Prometheus Dashboard
![Prometheus](./public/prometheus.JPG)

### WO! ooo!!! ...Your project is now deployed.
